# 角点检测

角点各方向像素值都会发生显著变化，边缘点只有一个方向变化明显，平坦区域则无明显变化

## Harris 角点检测

`dst = cv2.cornerHarris(src, blockSize, ksize, k[, dst[, borderType]])`

| 参数名          | 类型 / 取值范围                 | 核心作用                                                                  |
| ------------ | ------------------------- | --------------------------------------------------------------------- |
| `src`        | numpy.ndarray（float32 型）  | 输入图像，**必须是单通道灰度图**，且需转换为`float32`类型（不能直接用 uint8）。                     |
| `blockSize`  | int（正整数，如 2、3、5）          | 计算角点时的 “邻域窗口大小”：窗口越大，检测到的角点越少、越 “粗”（对噪声更鲁棒）；窗口越小，角点越多、越精细（但易受噪声影响）。   |
| `ksize`      | int（奇数，如 3、5、7）           | Sobel 梯度算子的核大小，用于计算图像 x/y 方向的梯度；值越大，梯度计算越平滑，角点检测越 “宽松”。               |
| `k`          | float（0.04~0.06 为主）       | Harris 算法的经验常数，用于计算角点响应值；k 越小，响应值越高，越容易检测到角点（但噪声误检率上升）；k 越大，角点越少、越精准。 |
| `borderType` | int（如 cv2.BORDER_DEFAULT） | 边界处理方式，默认即可，几乎无需调整。                                                   |
`dst`：与输入图像同尺寸的`float32`型二维数组，每个像素值代表该位置的**Harris 响应值**：

- 响应值为正且越大，该点是角点的概率越高；
- 响应值为负，该点大概率是边缘或平坦区域；
- 实际使用时需通过 “阈值筛选”（如`dst > 0.01*dst.max()`）提取真正的角点。

## Shi-Tomasi 角点检测

`corners = cv2.goodFeaturesToTrack(src, maxCorners, qualityLevel, minDistance[, corners[, mask[, blockSize[, useHarrisDetector[, k]]]]])`

| 参数名                 | 类型 / 取值范围                    | 核心作用                                                                                        |
| ------------------- | ---------------------------- | ------------------------------------------------------------------------------------------- |
| `src`               | numpy.ndarray（uint8/float32） | 输入图像，**单通道灰度图**（无需强制转 float32，uint8 即可）。                                                    |
| `maxCorners`        | int（0 或正整数）                  | 最多检测的角点数量：设为`0`则不限制数量；设为 100 则最多返回 100 个最优角点（按质量排序）。                                        |
| `qualityLevel`      | float（0~1，如 0.01、0.05）       | 角点 “质量阈值”：仅保留 “质量值≥最大质量值 ×qualityLevel” 的角点；值越小，角点越多（如 0.01 比 0.05 检测的角点多）；值越大，角点质量越高、数量越少。 |
| `minDistance`       | float（正整数，如 5、10、20）         | 两个角点之间的**最小像素距离**：值越大，角点分布越稀疏（避免同一区域密集检测）；值越小，角点越密集。                                        |
| `mask`              | numpy.ndarray（可选）            | 掩码图像（单通道，uint8 型），非零区域才会检测角点，用于限定检测范围（默认 None，全图检测）。                                        |
| `blockSize`         | int（正整数，默认 3）                | 计算梯度的邻域窗口大小，作用同 Harris 的`blockSize`，默认 3 即可满足多数场景。                                          |
| `useHarrisDetector` | bool（True/False，默认 False）    | 是否使用 Harris 算法替代 Shi-Tomasi：设为 True 则等价于 “带数量控制的 Harris 检测”，False 则用 Shi-Tomasi（推荐默认）。      |
| `k`                 | float（默认 0.04）               | 仅当`useHarrisDetector=True`时生效，作用同 Harris 的`k`参数。                                            |
`corners`：形状为`(N, 1, 2)`的`float32`型数组（N 为检测到的角点数量），每个元素是一个二维坐标`[[x, y]]`，代表角点的像素位置：

- 例如`corners[0] = [[100.5, 200.8]]`，表示第一个角点的坐标是 (x≈100.5, y≈200.8)；
- 实际使用时需通过`np.int0(corners)`转换为整数坐标，才能用于绘图 / 后续处理。

```python
import cv2 as cv
import numpy as np

# 滑动条
WINDOW_NAME = 'change'
cv.namedWindow(WINDOW_NAME, cv.WINDOW_NORMAL)
bars = [['blocksize', 5, 10], ['ksize', 2, 10], ['k/qualityLevel', 40, 100], ['thresholdratio', 1, 100], 
        ['maxCorners', 100, 800], ['minDistance', 10, 100]]
for (name, val, max) in bars:
    cv.createTrackbar(name, WINDOW_NAME, val, max, lambda x: None)

# 读图
img = cv.imread('./image/rili.jpg')
if img is None:
    print("图像读取失败，请检查路径！")
    exit(0)

# 转化为灰度图
gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)
gray = cv.GaussianBlur(gray, (5,5), 0)
gray_f = np.float32(gray)

while 1:
    img_copy = img.copy()
    img_copy1 = img.copy()
    vals = {}
    for name, _, _ in bars:
        vals[name] = cv.getTrackbarPos(name, WINDOW_NAME)

    # Harris
    dst = cv.cornerHarris(gray_f, vals['blocksize'], 2*vals['ksize']+1, 
                          vals['k/qualityLevel']/1000)

    dst = cv.dilate(dst, None)
    img_copy[dst >= dst.max() * vals['thresholdratio']/100] = (0, 255, 0)

    cv.imshow('img', img_copy)

    # Shi-Tomasi
    corners = cv.goodFeaturesToTrack(gray, vals['maxCorners'],
								     vals['k/qualityLevel']/1000, 
                                     vals['minDistance'])
    corners = np.int32(corners)  # 转换为整数坐标，整数类型要能覆盖图像分辨率

    for corner in corners:
        x, y = corner.ravel()
        cv.circle(img_copy1, (x, y), 2, (0, 255, 0), -1)
    cv.imshow('img1', img_copy1)
    cv.imshow('gray', gray)

    if cv.waitKey(30) == ord('q'):
        break
cv.destroyAllWindows()
```

## FAST 角点检测器

```python
# ===================== 步骤1：创建FAST检测器对象 =====================
# 核心API：cv2.FastFeatureDetector_create()
# 所有参数都有默认值，可按需调整
fast = cv2.FastFeatureDetector_create(
    threshold=20,          # 核心参数：灰度差异阈值
    nonmaxSuppression=True, # 是否启用非极大值抑制（推荐True，否则会有检测出密集点）
    type=cv2.FAST_FEATURE_DETECTOR_TYPE_9_16  # 检测模板类型，还有_5_8, _7_12
)

# ===================== 步骤2：执行角点检测 =====================
# detect()返回cv2.KeyPoint对象列表，每个对象包含角点的坐标、大小、响应值等信息，使用.调用
keypoints = fast.detect(gray, mask=None)  # mask：掩码图像，非零区域才检测，默认None

# ===================== 步骤3：结果可视化/后处理 =====================
# 绘制角点（两种绘制模式）
# 模式1：仅绘制角点坐标（简单圆点）
img_keypoints_simple = cv2.drawKeypoints(
    img, keypoints, None, 
    color=(0, 255, 0),  # BGR：绿色
    flags=cv2.DRAW_MATCHES_FLAGS_DEFAULT  # 默认模式：仅圆点
)

# 模式2：绘制角点+大小+方向（丰富模式，便于观察）但是这里无法显示方向，因为FAST不计算方向
img_keypoints_rich = cv2.drawKeypoints(
    img, keypoints, None, 
    color=(0, 0, 255),  # BGR：红色
    flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS
)
```

**例子**
```python
import cv2 as cv
import numpy as np

# 滑动条
WINDOW_NAME = 'FAST'
cv.namedWindow(WINDOW_NAME, cv.WINDOW_NORMAL)
cv.createTrackbar('threshold', WINDOW_NAME, 20, 100, lambda x: None)

# 创建检测器
fast = cv.FastFeatureDetector_create()
    
# 读取视频
cap = cv.VideoCapture(0)
if not cap.isOpened():
    exit(0)
while 1:
    ret, frame = cap.read()
    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)

    # 检测之前先降噪
    gray = cv.GaussianBlur(gray, (5, 5), 0)

    # 修改阈值
    val = cv.getTrackbarPos('threshold', WINDOW_NAME)
    fast.setThreshold(val)
    keyPoints = fast.detect(gray, mask=None)

    # 绘制点
    img1 = cv.drawKeypoints(frame, keyPoints, None, (0, 255, 0), 
                            cv.DRAW_MATCHES_FLAGS_DEFAULT)
    cv.imshow(WINDOW_NAME, img1)

    if cv.waitKey(50) == ord('q'):
        break
    
cap.release()
cv.destroyAllWindows()
```

# 特征描述子

特征点 的 描述子（描述像素点邻域内的特征）

| 描述子类型 | 全称                                            | 核心特点                                                                  | 适用场景                            |
| ----- | --------------------------------------------- | --------------------------------------------------------------------- | ------------------------------- |
| ORB   | Oriented FAST and Rotated BRIEF               | ① 基于 FAST 角点 + 改进版 BRIEF 描述子；② 旋转不变 + 尺度不变；③ 速度快（免费）；④ 二进制描述子（0/1 向量） | **最常用**，实时场景（如目标跟踪、无人机）、嵌入式设备   |
| SIFT  | Scale-Invariant Feature Transform             | ① 尺度 + 旋转 + 光照不变性极强；② 浮点型描述子（128 维）；③ 精度高但速度慢；④ 早期有专利（OpenCV4.4 + 免费） | **高精度场景**（如图像匹配、3D 重建）          |
| SURF  | Speeded Up Robust Features                    | ① SIFT 的加速版（快 3 倍）；② 尺度 + 旋转不变；③ 浮点型描述子（64/128 维）；④ 有专利               | **SIFT 加速版**，已逐渐被 ORB 替代，仅老项目使用 |
| BRIEF | Binary Robust Independent Elementary Features | ① 二进制描述子（64 维 0/1）；② 速度极快；③ 无旋转 / 尺度不变性                               | **基础**，简单场景、资源受限设备              |
| BRISK | Binary Robust Invariant Scalable Keypoints    | ① 二进制描述子；② 旋转 + 尺度不变；③ 速度快于 SIFT/SURF                                 | 平衡速度与不变性的场景                     |
## ORB

```python
# 创建对象
orb = cv2.ORB_create(
    nfeatures=500,        # 最多检测的特征点数量（默认500）
    scaleFactor=1.2,      # 尺度金字塔缩放因子（>1，越小尺度越细，默认1.2）
    patchSize=31,         # 特征点邻域大小（描述子提取范围，默认31）
    edgeThreshold=31      # 边缘阈值（避免检测图像边缘的点，默认31）
)

# 检测特征点 + 计算描述子
kp = orb.detect(img_gray, None)
des = orb.compute(img_gray, kp)
kp, des = orb.detectAndCompute(img_gray, None)
```

## SIFT

```python
# 创建
sift = cv2.SIFT_create(
    nfeatures=500,        # 最多特征点数量（默认0=不限制）
    contrastThreshold=0.04,  # 对比度阈值（过滤低对比度噪声点，默认0.04）
    edgeThreshold=10,     # 边缘阈值（过滤边缘点，默认10）
    sigma=1.6             # 高斯核标准差（尺度金字塔的初始平滑，默认1.6）
)
# 检测特征点 + 计算描述子 同上
```

## SURF

```python
# hessianThreshold：海森矩阵阈值，越大特征点越少、质量越高
surf = cv2.xfeatures2d.SURF_create(hessianThreshold=400)
```
商用场景避免使用 SURF（专利问题），优先用 ORB/SIFT

## BRIEF

仅为 “描述子生成器”，**无自带特征检测器**，需手动结合 FAST/Harris 等角点检测器

```python
# =====================步骤1：用FAST检测角点（BRIEF无检测器） =====================
fast = cv2.FastFeatureDetector_create(threshold=20, nonmaxSuppression=True)
kp1 = fast.detect(img1, None)
kp2 = fast.detect(img2, None)

# =====================步骤2：创建BRIEF描述子生成器 =====================
# bytes=32 → 64维二进制描述子（1字节=8位，32*8=256, OpenCV中bytes=8对应64位）
brief = cv2.xfeatures2d.BriefDescriptorExtractor_create(bytes=8)

# =====================步骤3：计算BRIEF描述子 =====================
kp1, des1 = brief.compute(img1, kp1) # 这里的kp和fast.detect的kp一样，无本质区别
kp2, des2 = brief.compute(img2, kp2)
```

## 例子

```python
import cv2 as cv

img = cv.imread('./image/horse.jpg')
gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)

orb = cv.ORB_create()
sift = cv.SIFT_create()
# surf = cv.xfeatures2d.SURF_create() # 专利保护
fast = cv.FastFeatureDetector_create()
brief = cv.xfeatures2d.BriefDescriptorExtractor_create()

kp_o = orb.detect(gray, None)
kp_si = sift.detect(gray, None)
# kp_su, des = surf.detectAndCompute(gray, None)
kp_f = fast.detect(gray, None)
kp_b, des = brief.compute(gray, kp_f)

img_o = cv.drawKeypoints(img, kp_o, None, (0, 255, 0), 
                                 cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)
img_si = cv.drawKeypoints(img, kp_si, None, (0, 255, 0), 
                                 cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)
# img_su = cv.drawKeypoints(img, kp_su, None, (0, 255, 0), 
#                                  cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)
img_b = cv.drawKeypoints(img, kp_b, None, (0, 255, 0), 
                                 cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS
                                 # 无法展现方向
                                 )

cv.imshow('orb', img_o)
cv.imshow('sift', img_si)
# cv.imshow('surf', img_su)
cv.imshow('brief', img_b)

cv.waitKey(0)
cv.destroyAllWindows()
```
![[Pasted image 20251228154537.png]]



# 特征匹配

图像读取 → 特征提取 → 特征匹配 → 初步过滤（比率测试/距离阈值） → RANSAC拟合矩阵 → 筛选内点 → 结果可视化

**距离度量**：判断两个描述子是否相似（距离越小，相似度越高）
- 针对**浮点型描述子**（如 SIFT、SURF）：用**欧氏距离（L2）** 或曼哈顿距离；
- 针对**二进制描述子**（如 ORB、BRIEF）：用**汉明距离**（统计二进制位不同的个数，汉明距离越小越相似）。
**匹配目标**：对图像 A 中的每个特征点，在图像 B 中找到 “最像” 的特征点，同时过滤掉错误匹配（误匹配）。

`DMatch`对象：

|属性名|含义|
|---|---|
|`queryIdx`|图像 1（查询图）中特征点的索引（即`kp1[queryIdx]`是该匹配对应的特征点）|
|`trainIdx`|图像 2（训练图）中特征点的索引（即`kp2[trainIdx]`是匹配的特征点）|
|`imgIdx`|多图匹配时，训练图的索引（单图匹配时固定为 0）|
|`distance`|两个描述子的距离（汉明 / 欧氏），越小匹配质量越高|

## 暴力匹配（Brute-Force Matcher，BF 匹配）

```python
'''1 初始化BF匹配器'''
# 对二进制描述子：normType=cv2.NORM_HAMMING；
# 对浮点型（SIFT）：normType=cv2.NORM_L2
bf = cv2.BFMatcher(normType=cv2.NORM_HAMMING, crossCheck=True)  
# crossCheck=True：双向验证，提升匹配精度

'''2 执行匹配'''
matches = bf.match(des1, des2)

'''3 按距离排序（距离越小，匹配越优）'''
matches = sorted(matches, key=lambda x: x.distance)
```

- `crossCheck=True`：开启 “交叉验证”—— 仅保留 “图像 A 的点 i 匹配图像 B 的点 j，且图像 B 的点 j 也匹配图像 A 的点 i” 的匹配对，能大幅减少误匹配（但会损失部分正确匹配）；

`matches` 是长度为 匹配特征点数量，元素为`DMatch`对象的列表


## 快速最近邻匹配（FLANN Matcher）

通过构建高效索引（如 KD-Tree、K-Means 树），避免暴力遍历，大幅提升匹配速度（比 BF 快 10~100 倍）。
仅适用于**浮点型描述子**（SIFT、SURF）；二进制描述子（ORB）用 FLANN 效果差，建议用 BF

```python
'''FLANN参数设置'''
FLANN_INDEX_KDTREE = 1
index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)  # 索引参数：KD树，5棵树
search_params = dict(checks=50)  # 搜索参数：检查50次（值越大越准，速度越慢）

'''初始化FLANN匹配器并匹配'''
flann = cv2.FlannBasedMatcher(index_params, search_params)
matches = flann.knnMatch(des1, des2, k=2)  # knnMatch：取每个点的前2个最近邻
```

**核心优化：Lowe's 比率测试**

FLANN 通常用`knnMatch(k=2)`取每个点的前 2 个最近邻，通过 “比率测试” 过滤误匹配：

- 若最近邻（m）的距离远小于次近邻（n）的距离（如 `m.distance` < `0.75n.distance`），说明匹配唯一且可靠；
- 若两者距离接近，说明该特征点在另一幅图中有多个相似点，大概率是误匹配，直接丢弃。

`matches` 是长度为 匹配特征点数量，元素为长度 = k（通常 k=2），每个元素是`DMatch`对象的列表，即它是二维列表



## 匹配优化

| 策略               | 适用场景                  | 原理                                   |
| ---------------- | --------------------- | ------------------------------------ |
| Lowe's 比率测试      | FLANN/knnMatch        | 筛选 “唯一相似” 的匹配对                       |
| 交叉验证（crossCheck） | BF 匹配                 | 双向验证匹配对的唯一性                          |
| 距离阈值过滤           | 所有匹配方式                | 设定最大距离阈值（如取所有匹配距离的中位数的 2 倍），过滤超阈值的匹配 |
| 几何约束（RANSAC）     | 有仿射 / 透视变换的场景（如拼接、定位） | 基于匹配对的坐标，拟合变换矩阵，剔除不符合变换的误匹配          |

### Lowe’s ratio test

```python
matches = bf.knnMatch(des1,des2,k=2)
# 应用比率测试
good = []
for m,n in matches
    if m.distance < 0.75*n.distance
good.append([m])
img3 = cv.drawMatchesKnn(img1,kp1,img2,kp2,good,None,
						flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)
```

```python
# 需要仅绘制良好的匹配项，因此请创建一个掩码
matchesMask = [[0,0] for i in range(len(matches))]
# 根据 Lowe 的论文进行比率测试
for i,(m,n) in enumerate(matches)
    if m.distance < 0.7*n.distance
matchesMask[i]=[1,0]
 
draw_params = dict(matchColor = (0,255,0),
					singlePointColor = (255,0,0),
					matchesMask = matchesMask,
					flags = cv.DrawMatchesFlags_DEFAULT)
 
img3 = cv.drawMatchesKnn(img1,kp1,img2,kp2,matches,None,**draw_params)
```
### 几何约束：RANSAC

```python
# ===================== 步骤5：RANSAC拟合单应性矩阵 =====================
# 核心参数解释：
# - method=cv2.RANSAC：启用RANSAC鲁棒拟合
# - ransacReprojThreshold=5.0：重投影误差阈值（像素），误差<5则为内点
# - maxIters=2000：RANSAC最大迭代次数
# - confidence=0.995：99.5%概率找到最优模型
H, mask = cv2.findHomography(
    srcPoints=src_pts,
    dstPoints=dst_pts,
    method=cv2.RANSAC,
    ransacReprojThreshold=5.0,
    maxIters=2000,
    confidence=0.995
)

# 提取内点掩码：mask是N×1的uint8数组，1=内点（正确匹配），0=外点（误匹配）
matches_mask = mask.ravel().tolist()  # 转成列表，适配drawMatches

# ================= 步骤6：可视化结果（仅绘制RANSAC筛选后的内点） ==================
# 定义绘制参数
draw_params = dict(
    matchColor=(0, 255, 0),    # 内点：绿色
    singlePointColor=None,     # 不绘制孤立点
    matchesMask=matches_mask,  # 只绘制内点
    flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS
)

# 绘制匹配结果
img_matches = cv2.drawMatches(
    img1, kp1, img2, kp2, good_matches, None, **draw_params
)
```

### 距离阈值过滤

#### BF 匹配（match ()）的距离阈值过滤

```python
# 方法1：统计法（推荐）——取中位数×1.5作为阈值
distances = [m.distance for m in matches]
threshold = np.median(distances) * 1.5  # 动态阈值

# 方法2：经验法（固定值）——ORB的汉明距离阈值设为40
threshold = 40

# 筛选符合阈值的匹配对
good_matches = [m for m in matches if m.distance < threshold]
```

#### knnMatch（FLANN/SIFT）的距离阈值过滤

```python
# 双层过滤：Lowe比率测试 + 距离阈值过滤
good_matches = []
# 先算所有有效匹配（通过比率测试）的距离，用于统计阈值
valid_distances = []
for m, n in matches:
    if m.distance < 0.75 * n.distance:  # Lowe比率测试
        valid_distances.append(m.distance)

# 动态计算距离阈值（均值+1倍标准差）
if valid_distances:
    threshold = np.mean(valid_distances) + np.std(valid_distances)
    # 二次过滤：比率测试通过 + 距离小于阈值
    for m, n in matches:
        if m.distance < 0.75 * n.distance and m.distance < threshold:
            good_matches.append(m)
```




# 相机标定

固定标定图像的世界坐标，通过多张图片找出对应的图像坐标，得出相机内参

## 找出图像坐标
`ret, corners = cv2.findChessboardCorners(image, patternSize, flags=None)`

| 参数            | 类型 / 取值                                                                                                                                                                                                                  | 含义与注意事项                                                                                              |
| ------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------- |
| `image`       | 灰度图像（`np.ndarray`）                                                                                                                                                                                                       | 输入图像，必须是单通道灰度图（需先用`cv2.cvtColor`转换），彩色图会报错；建议图像清晰、光照均匀                                               |
| `patternSize` | 元组 `(cols, rows)`                                                                                                                                                                                                        | 棋盘格**内角点的行列数**（如 8×5 表示 8 列、5 行内角点），是最容易出错的参数！<br><br>⚠️ 注意：不是棋盘格的格子数（比如 9×6 的格子，内角点是 8×5）           |
| `flags`       | 可选标志（可组合）：<br><br>- `cv2.CALIB_CB_ADAPTIVE_THRESH`：自适应阈值<br><br>- `cv2.CALIB_CB_FAST_CHECK`：快速检测（先判断是否有棋盘格，无则直接返回）<br><br>- `cv2.CALIB_CB_NORMALIZE_IMAGE`：归一化图像（均衡亮度）<br><br>- `cv2.CALIB_CB_FILTER_QUADS`：过滤错误的四边形检测结果 | 推荐组合：`cv2.CALIB_CB_ADAPTIVE_THRESH + cv2.CALIB_CB_FAST_CHECK + cv2.CALIB_CB_NORMALIZE_IMAGE`，提高检测鲁棒性 |
| **返回值**       | **类型**                                                                                                                                                                                                                   | **含义**                                                                                               |
| `ret`         | 布尔值（True/False）                                                                                                                                                                                                          | 检测是否成功：<br><br>- `True`：找到所有内角点；<br><br>- `False`：漏检 / 错检，需检查图像或`patternSize`                        |
| `corners`     | 数组（`(N, 1, 2)`）                                                                                                                                                                                                          | 成功时：N 个角点的像素坐标，形状为`(内角点总数, 1, 2)`，每个元素是`(x, y)`像素坐标；<br><br>失败时：空数组                                  |

## 亚像素级细化坐标

`corners_refined = cv2.cornerSubPix(image, corners, winSize, zeroZone, criteria)`

| 参数                | 类型 / 取值                            | 含义与注意事项                                                                                                                                                            |
| ----------------- | ---------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `image`           | 灰度图像（`np.ndarray`）                 | 与`cv2.findChessboardCorners`的输入灰度图一致                                                                                                                               |
| `corners`         | 数组（`(N, 1, 2)`）                    | `cv2.findChessboardCorners`检测到的原始角点坐标                                                                                                                              |
| `winSize`         | 元组 `(w, h)`（奇数，如`(11,11)`）         | 亚像素细化的窗口大小，窗口越大，细化越精准，但速度稍慢；推荐`(11,11)`或`(9,9)`                                                                                                                    |
| `zeroZone`        | 元组 `(x, y)`（通常设为`(-1,-1)`）         | 窗口中忽略的中心区域（避免像素梯度为 0 的影响），`(-1,-1)`表示无忽略区域                                                                                                                         |
| `criteria`        | 迭代终止条件：`(type, max_iter, epsilon)` | - `type`：终止类型，常用`cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER`（满足迭代次数或精度则停止）；<br><br>- `max_iter`：最大迭代次数（如 30）；<br><br>- `epsilon`：精度阈值（如 0.001，坐标变化小于该值则停止） |
| `corners_refined` | 数组（`(N, 1, 2)`）                    | 细化后的亚像素级角点坐标，数据类型为`float32`（原始角点是`float32`，但值为整数）                                                                                                                  |


## 执行相机标定

`ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objectPoints, imagePoints, imageSize, cameraMatrix=None, distCoeffs=None)`

| 参数             | 类型 / 取值                 | 含义与注意事项                                                                               |
| -------------- | ----------------------- | ------------------------------------------------------------------------------------- |
| `objectPoints` | 列表（元素为`np.ndarray`）     | 每个元素是单张图像的 3D 世界坐标数组（`(N, 3)`，`float32`），长度 = 标定图像数量；<br><br>通常所有图像的 3D 坐标相同（因为棋盘格固定） |
| `imagePoints`  | 列表（元素为`np.ndarray`）     | 每个元素是单张图像的 2D 像素坐标数组（`(N, 1, 2)`，`float32`），长度 = 标定图像数量；<br>需与`objectPoints`一一对应      |
| `imageSize`    | 元组 `(width, height)`    | 图像的尺寸（宽度 × 高度），可通过`gray.shape[::-1]`获取（灰度图 shape 是`(h,w)`，反转后是`(w,h)`）                |
| `cameraMatrix` | 3×3 矩阵（可选，默认`None`）     | 初始内参矩阵：<br><br>- `None`：由算法自动求解；<br><br>- 若已知初始值（如前一次标定结果），可传入以加速收敛                   |
| `distCoeffs`   | 1×5/1×8 数组（可选，默认`None`） | 初始畸变系数：<br><br>- `None`：由算法自动求解；<br><br>- 格式：`[k1,k2,p1,p2,k3]`（5 参数）或扩展的 8 参数        |
| **返回值**        | **类型**                  | **含义**                                                                                |
| `ret`          | 浮点数                     | 标定的**重投影误差的总平方和**（越小越好），可作为标定成功的参考（但不如平均重投影误差精准）                                      |
| `mtx`          | 3×3 矩阵（`float32`）       | ![[Pasted image 20251230110554.png]]<br>                                              |
| `dist`         | 1×5 数组（`float32`）       | 畸变系数 `[k1,k2,p1,p2,k3]`：<br><br>- \(k1,k2,k3\)：径向畸变系数；<br><br>- \(p1,p2\)：切向畸变系数      |
| `rvecs`        | 列表（元素为 3×1 数组）          | 每张图像的**旋转向量**（Rodrigues 向量），描述相机相对于世界坐标系的旋转；<br><br>可通过`cv2.Rodrigues`转为 3×3 旋转矩阵     |
| `tvecs`        | 列表（元素为 3×1 数组）          | 每张图像的**平移向量**，描述相机相对于世界坐标系的平移（单位：与棋盘格尺寸一致，如 mm）                                       |


# PnP位姿估计

```python
success, rvec, tvec = cv2.solvePnP(
    objectPoints,
    imagePoints,
    cameraMatrix,
    distCoeffs,
    rvec=None,
    tvec=None,
    useExtrinsicGuess=False,
    flags=cv2.SOLVEPNP_EPNP
)

# RANSAC版本
success, rvec, tvec, inliers = cv2.solvePnPRansac(
    objectPoints,
    imagePoints,
    cameraMatrix,
    distCoeffs,
    flags=cv2.SOLVEPNP_EPNP,
    reprojectionError=1.5,  # 内点阈值：重投影误差≤1.5像素
    iterationsCount=100,    # RANSAC迭代次数
    confidence=0.99         # 成功率（99%概率找到最优模型）
)
# inliers是内点的索引
```

| 参数名            | 数据类型 / 形状                   | 含义与注意事项                                                                                                                                                |
| -------------- | --------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `objectPoints` | np.float32，(n, 3) 或 (n,1,3) | 3D 世界点集（n≥4，推荐≥6）：<br><br>- 比如棋盘格角点，世界坐标系下坐标（常设 Z=0）；<br><br>- 必须是 `float32` 类型，不能用 int；<br><br>- 示例：`np.array([[0,0,0], [20,0,0]], dtype=np.float32)` |
| `imagePoints`  | np.float32，(n, 2) 或 (n,1,2) | 2D 图像点集：<br><br>- 与 3D 点一一对应（第 i 个 3D 点对应第 i 个 2D 点）；<br><br>- 必须是 `float32` 类型；<br><br>- 来源：角点检测（`cv2.findChessboardCorners`）、特征匹配等                   |
| `cameraMatrix` | np.float32，(3, 3)           | 相机内参矩阵 K（单目标定得到）：<br><br>\(K=\begin{bmatrix}f_x & 0 & c_x \\ 0 & f_y & c_y \\ 0 & 0 & 1\end{bmatrix}\)                                                 |
| `distCoeffs`   | np.float32，(5,) / None      | 畸变系数 D：<br><br>- 格式：`[k1, k2, p1, p2, k3]`（径向 + 切向畸变）；<br><br>- 无畸变传 `None` 或 `np.zeros((5,), dtype=np.float32)`                                       |
| `success`      | 布尔值                         | True = 求解成功，False = 失败（如点太少、点共线）                                                                                                                       |
```python
points2d, jacobian = cv2.projectPoints(
    objectPoints,
    rvec,
    tvec,
    cameraMatrix,
    distCoeffs,
    # 可选参数
    rvec2=None,
    tvec2=None,
    J=None,
    aspectRatio=1.0
)
```

| 参数名            | 数据类型 / 形状                      | 含义与注意事项                                                                                                                                                        |
| -------------- | ------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `objectPoints` | np.float32，形状 (n, 3) 或 (n,1,3) | 待投影的 3D 点集（n 个点）：<br><br>- 世界坐标系 / 相机坐标系均可（取决于 rvec/tvec 的定义）；<br><br>- 必须是 float32 类型（不能是 int）；<br><br>- 示例：`np.array([[0,0,0], [20,0,0]], dtype=np.float32)` |
| `rvec`         | np.float32，形状 (3,) 或 (3,1)     | 旋转向量（Rodrigues 形式）：<br><br>- 描述 3D 点坐标系到相机坐标系的旋转；<br><br>- 注意：不是旋转矩阵（需用`cv2.Rodrigues()`转换）；<br><br>- PnP 求解的`rvec`可直接传入                                       |
| `tvec`         | np.float32，形状 (3,) 或 (3,1)     | 平移向量：<br><br>- 描述 3D 点坐标系到相机坐标系的平移（单位：通常 mm / 像素，与内参一致）；<br><br>- PnP 求解的`tvec`可直接传入                                                                           |
| `cameraMatrix` | np.float32，形状 (3,3)            | 相机内参矩阵 K：<br>由相机标定得到，必须是 float32                                                                                                                               |
| **返回值**        | **数据类型 / 形状**                  | **含义**                                                                                                                                                         |
| `points2d`     | np.float32，形状 (n, 1, 2)        | 投影后的 2D 像素点集：<br><br>- 每个点是`(u, v)`坐标；<br><br>- 需用`.squeeze()`转为 (n,2) 方便计算                                                                                    |
| `jacobian`     | np.float32 或 None              | 雅可比矩阵（优化用，普通场景可忽略）                                                                                                                                             |
```python
R, _ = cv2.Rodrigues(rvec)  # 旋转向量→旋转矩阵
```

```python
import cv2
import numpy as np

# 1. 准备输入
# 3D世界点：棋盘格角点（假设棋盘格每个格子20mm，共3×3角点）
object_points = np.array([
    [0, 0, 0], [20, 0, 0], [40, 0, 0],
    [0, 20, 0], [20, 20, 0], [40, 20, 0],
    [0, 40, 0], [20, 40, 0], [40, 40, 0]
], dtype=np.float32)

# 2D图像点：假设已检测到的角点像素坐标
image_points = np.array([
    [100, 100], [150, 100], [200, 100],
    [100, 150], [150, 150], [200, 150],
    [100, 200], [150, 200], [200, 200]
], dtype=np.float32)

# 相机内参（单目标定得到）
K = np.array([
    [800, 0, 320],   # fx, 0, cx
    [0, 800, 240],   # 0, fy, cy
    [0, 0, 1]        # 0, 0, 1
], dtype=np.float32)

# 畸变系数（单目标定得到，无畸变则为全0）
D = np.array([0, 0, 0, 0, 0], dtype=np.float32)

# 2. 调用solvePnP求解位姿
# 参数：SOLVEPNP_EPNP（高效PnP），输出旋转向量rvec（Rodrigues形式）、平移向量tvec
success, rvec, tvec = cv2.solvePnP(
    objectPoints=object_points,
    imagePoints=image_points,
    cameraMatrix=K,
    distCoeffs=D,
    flags=cv2.SOLVEPNP_EPNP
)

# 3. 旋转向量rvec转旋转矩阵R（Rodrigues变换）
R, _ = cv2.Rodrigues(rvec)
print("旋转矩阵R：\n", R)
print("平移向量tvec：\n", tvec)
```


# 双目视差和深度估计

```python
import cv2
import numpy as np

# 1. 读取双目图像（左、右视图，需先做校正/去畸变）
imgL = cv2.imread('left.jpg', 0)  # 灰度图读取，0表示灰度模式
imgR = cv2.imread('right.jpg', 0)

# 2. 创建StereoSGBM实例（核心参数配置）
sgbm = cv2.StereoSGBM_create(
    minDisparity=0,        # 最小视差
    numDisparities=160,    # 视差范围（必须是16的整数倍）
    blockSize=5,           # 匹配块大小（奇数，3-11常用）
    P1=8*3*5**2,           # 正则化参数1
    P2=32*3*5**2,          # 正则化参数2
    disp12MaxDiff=1,       # 左右视差检查最大差值
    uniquenessRatio=10,    # 唯一性检测阈值
    speckleWindowSize=100, # 斑点去除窗口大小
    speckleRange=32,       # 斑点去除范围
    mode=cv2.STEREO_SGBM_MODE_HH  # 匹配模式
)

# 3. 计算视差图
disparity = sgbm.compute(imgL, imgR)

# 4. 视差图后处理（归一化，方便可视化）
disparity = (disparity - minDisparity) / numDisparities  # 归一化到0~1
disparity = (disparity * 255).astype(np.uint8)  # 转换为8位灰度图

# 5. 显示/保存结果
cv2.imshow('Disparity Map', disparity)
cv2.waitKey(0)
cv2.destroyAllWindows()
```

| `minDisparity`      | 最小视差值（视差的起始值）                                                      | 通常设为 0；若左 / 右视图有偏移，可设为负数（如 - 16），但需保证视差非负                                                                                    |
| ------------------- | ------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------- |
| `numDisparities`    | 视差范围（最大视差 = minDisparity+numDisparities），**必须是 16 的整数倍**           | 越小计算越快，但可能丢失大视差区域；越大覆盖范围越广，但计算量 / 内存占用越高<br><br>（建议：先试 16/32/64/128，根据场景调整）                                                  |
| `blockSize`         | 匹配块大小（奇数，≥3）                                                       | 小值（3/5）：精度高、噪声大，适合细节丰富场景（如小物体）<br><br>大值（7/9/11）：平滑、噪声小，细节丢失多，适合大场景                                                          |
| `P1`/`P2`           | 正则化参数（控制视差平滑度）<br><br>P1：相邻像素视差变化≤1 的惩罚<br><br>P2：相邻像素视差变化 > 1 的惩罚 | 固定公式：<br><br>P1 = 8 \* 通道数（灰度图 = 1，彩色图 = 3） \* blockSize²<br><br>P2 = 32 \* 通道数 \* blockSize²<br><br>（P2 必须 > P1，P2 越大视差越平滑） |
| `disp12MaxDiff`     | 左右视差一致性检查的最大允许差值                                                   | 设为 - 1 关闭检查（速度快，噪声大）；设为 1~5（严格检查，剔除错误匹配）                                                                                     |
| `uniquenessRatio`   | 唯一性检测阈值（5~15 常用）                                                   | 值越大：匹配越严格，剔除模糊匹配，但可能丢失有效视差<br><br>值越小：匹配越宽松，噪声越多                                                                             |
| `speckleWindowSize` | 斑点去除的窗口大小（0 关闭）                                                    | 建议设 50~200，去除小面积的噪声视差区域                                                                                                      |
| `speckleRange`      | 斑点去除的视差范围阈值                                                        | 建议设 16~32，值越大去除的噪声越多                                                                                                         |
| `mode`              | 匹配模式                                                               | `STEREO_SGBM_MODE_SGBM`：速度快，精度一般<br><br>`STEREO_SGBM_MODE_HH`：精度高，速度慢（推荐）                                                    |
OpenCV 的 SGBM 为了提高精度，会把实际视差值（浮点数）放大 16 倍后用 int16 存储

```python
import cv2
import numpy as np

imgL = cv2.imread('left.jpg', 0)  # 灰度图读取，0表示灰度模式
imgR = cv2.imread('right.jpg', 0)

def make_stereo_SGBM(minDisparity=0, numDisparities=128, blockSize=5, channels=1):
    numDisparities = max(16, (numDisparities + 15) // 16 * 16) # 向上取16倍数

    sgbm = cv2.StereoSGBM_create(
        minDisparity=minDisparity,        # 最小视差
        numDisparities=numDisparities,    # 视差范围（必须是16的整数倍）
        blockSize=blockSize,           # 匹配块大小（奇数，3-11常用）
        P1=8*channels*blockSize**2,           # 正则化参数1
        P2=32*channels*blockSize**2,          # 正则化参数2
        disp12MaxDiff=1,       # 左右视差检查最大差值
        uniquenessRatio=10,    # 唯一性检测阈值
        speckleWindowSize=100, # 斑点去除窗口大小
        speckleRange=32,       # 斑点去除范围
        mode=cv2.STEREO_SGBM_MODE_HH  # 匹配模式
    )
    return sgbm

def normalize_disp_for_display(disp):
    disp = disp.astype(np.float32) / 16.0
    disp[disp < 0] = 0
    disp = cv2.normalize(disp, None, 0, 255, cv2.NORM_MINMAX)
    disp = np.uint8(disp)
    return disp

def main(scale):
    capR = cv2.VideoCapture('image/camera_right.mp4')
    capL = cv2.VideoCapture('image/camera_left.mp4')
    if not(capR.isOpened() and capL.isOpened()):
        exit("视频读取失败")
    
    # 获取视频信息
    fpsL = capL.get(cv2.CAP_PROP_FPS)
    fpsR = capR.get(cv2.CAP_PROP_FPS)
    fps = min(fpsL, fpsR)

    w = capL.get(cv2.CAP_PROP_FRAME_WIDTH)
    h = capL.get(cv2.CAP_PROP_FRAME_HEIGHT)
    out_w = int(w * scale)
    out_h = int(h * scale)

    # 输出视频写入器
    fourcc = cv2.VideoWriter_fourcc(*'mp4v') # 编码格式
    out = cv2.VideoWriter('./disparity_output_n.mp4', fourcc, fps, (out_w, out_h))

    sgbm = make_stereo_SGBM()
    frame_id = 0

    try:
        while cv2.waitKey(1) != ord('q'):
            (retR, frameR), (retL, frameL) = capR.read(), capL.read()

            if not (retR or retL):
                print('视频结束，停止处理')
                break

            # 缩放
            frameL = cv2.resize(frameL, (out_w, out_h), interpolation=cv2.INTER_AREA)
            frameR = cv2.resize(frameR, (out_w, out_h), interpolation=cv2.INTER_AREA)
            # 转灰度
            grayL = cv2.cvtColor(frameL, cv2.COLOR_BGR2GRAY)
            grayR = cv2.cvtColor(frameR, cv2.COLOR_BGR2GRAY)

            # 计算并可视化视差
            disp = sgbm.compute(grayL, grayR)
            disp = normalize_disp_for_display(disp)
            disp_color = cv2.applyColorMap(disp, cv2.COLORMAP_JET)

            # 写入输出视频
            out.write(disp_color)

            # 展示
            top = np.hstack((frameL, frameR))
            bottom = np.hstack((cv2.cvtColor(disp, cv2.COLOR_GRAY2BGR), disp_color))
            vis = np.vstack((top, bottom))
            cv2.imshow('Left | Right -- Disparity(gray) | Disparity(color)', vis)

            frame_id += 1
            if frame_id % 50 == 0:
                print(f'已处理帧数：{frame_id}')
    
    finally:
        cv2.destroyAllWindows()
        capL.release()
        capR.release()
        out.release()

if __name__ == '__main__':
    main(0.6)
```


# 视觉标签检测

**大致流程：**
标记准备 → 标记检测 → 位姿估计 → 可视化 / 应用

| 维度   | ArUco（OpenCV 内置）                | AprilTag（独立库，需单独安装）                 |
| ---- | ------------------------------- | ----------------------------------- |
| 所属生态 | OpenCV 官方模块（`cv2.aruco`），无需额外安装 | 独立开源库（C++/Python 接口），需安装`apriltag`包 |
| 标记形态 | 正方形边框 + 内部二进制编码点阵（如 4x4、5x5 编码） | 正方形边框 + 内部黑白棋盘格编码（如 36h11、25h9）     |
| 识别速度 | 快（OpenCV 底层优化）                  | 稍慢，但检测精度更高（抗模糊、遮挡更强）                |
| 编码容量 | 中等（如 4x4 编码支持 1024 个唯一标记）       | 大（如 36h11 编码支持 587 个标记，抗误码更强）       |
| 适用场景 | 实时性要求高（如 AR、机器人实时定位）            | 高精度定位（如工业检测、无人机导航）                  |
| 易用性  | 极高（OpenCV 接口统一，直接调用）            | 中等（需单独配置，Python 绑定较简单）              |
## ArUco

```python
cv2.aruco.Dictionary = cv2.aruco.getPredefinedDictionary(dictId)
```

|项|说明|
|---|---|
|参数 `dictId`|字典 ID（OpenCV 预定义常量），代表不同的编码规则和标记数量，常用值如下：<br><br>✅ `cv2.aruco.DICT_4X4_100`：4×4 编码矩阵，100 个唯一标记（速度快，推荐入门）；<br><br>✅ `cv2.aruco.DICT_5X5_50`：5×5 编码矩阵，50 个唯一标记；<br><br>✅ `cv2.aruco.DICT_7X7_1000`：7×7 编码矩阵，1000 个唯一标记（多目标场景）；<br><br>❌ 其他：如 DICT_6X6_250、DICT_ARUCO_ORIGINAL 等|
|返回值|选中的 ArUco 字典对象（后续生成 / 检测标记需传入此对象）|

```python
corners, ids, rejectedImgPoints = cv2.aruco.detectMarkers(
    image,
    dictionary,
    cameraMatrix=None,
    distCoeff=None,
    parameters=None,
    rejectedCorners=None
)
```

| 参数名                 | 类型 / 形状                      | 核心含义与注意事项                                                                                                                                        |
| ------------------- | ---------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ |
| **必选参数**            |                              |                                                                                                                                                  |
| `image`             | np.array（灰度图）                | 输入图像：**必须是单通道灰度图**（不能直接传彩色图，需用`cv2.cvtColor`转换）；<br><br>图像格式推荐：8 位灰度图（uint8）                                                                     |
| `dictionary`        | cv2.aruco.Dictionary         | 预定义字典对象（由`aruco.getPredefinedDictionary()`得到）；<br><br>必须与生成标记时的字典一致（如都用 DICT_4X4_100）                                                            |
| **可选参数（关键）**        |                              |                                                                                                                                                  |
| `cameraMatrix`      | np.float32 (3,3) / None      | 相机内参矩阵 K：<br><br>- 传入后，函数会先对图像做畸变矫正，提升角点检测精度；<br><br>- 无内参则传 None（检测精度略降）                                                                        |
| `distCoeff`         | np.float32 (5,) / None       | 相机畸变系数 D：<br><br>- 需与`cameraMatrix`配套传入；<br><br>- 无畸变传 None 或全 0 数组                                                                              |
| `parameters`        | cv2.aruco.DetectorParameters | 检测参数配置（如角点细化、阈值等）；<br>默认值可满足大部分场景                                                                                                                |
| `rejectedCorners`   | list / None                  | 兼容旧版本参数，无需手动传入，结果会通过`rejectedImgPoints`返回                                                                                                        |
| **返回值**             | **类型 / 形状**                  | **核心含义**                                                                                                                                         |
| `corners`           | list of np.array             | 有效标记的角点列表：<br><br>- 列表长度 = 检测到的标记数量；<br><br>- 每个元素是`(1, 4, 2)`形状的 float32 数组；<br><br>- 数组内 4 个角点顺序固定：`左上 → 右上 → 右下 → 左下`；<br><br>- 无有效标记时为`None` |
| `ids`               | np.array (n,1) / None        | 有效标记的 ID 列表：<br><br>- 形状为`(n,1)`（n = 标记数量），每个元素是标记的唯一整数 ID；<br><br>- 与`corners`一一对应（第 i 个 ID 对应第 i 个角点组）；<br><br>- 无有效标记时为`None`                 |
| `rejectedImgPoints` | list of np.array             | 被拒绝的候选标记角点：<br><br>- 这些区域形状像 ArUco 标记，但编码校验失败（如遮挡、模糊导致编码错误）；<br><br>- 格式与`corners`一致，可用于调试（如查看为什么某些标记没检测到）                                       |


```python
rvecs, tvecs, _objPoints = aruco.estimatePoseSingleMarkers(
    corners,
    markerLength,
    cameraMatrix,
    distCoeffs,
    rvecs=None,
    tvecs=None
)
```

| 参数名            | 类型 / 形状                | 核心含义与注意事项                                                                             |
| -------------- | ---------------------- | ------------------------------------------------------------------------------------- |
| `corners`      | list of np.array       | 从`aruco.detectMarkers()`得到的标记角点列表：<br><br>每个元素是`(1,4,2)`数组，对应一个标记的 4 个角点（左上→右上→右下→左下） |
| `markerLength` | float                  | ArUco 标记的**物理边长**（单位：米！如 5 厘米标记传`0.05`），尺寸错误会导致位姿结果偏差                                 |
| `cameraMatrix` | np.float32 (3,3)       | 相机内参矩阵 K（必须是标定得到的真实值，示例：`[[800,0,320],[0,800,240],[0,0,1]]`）                          |
| `distCoeffs`   | np.float32 (5,) / None | 相机畸变系数 D：<br><br>- 有畸变传标定值（如`[k1,k2,p1,p2,k3]`）；<br><br>- 无畸变传`None`或全 0              |
| `rvecs/tvecs`  | np.array（可选）           | 位姿初始猜测（极少用，默认 None 即可）                                                                |
| `_objPoints`   | np.float32 (n,4,3)     | 函数内部构造的标记 3D 点（极少用到，可忽略）                                                              |
| `rvecs`        | np.float32 (n,1,3)     | 每个标记的旋转向量（Rodrigues 形式）：<br><br>- `n`是检测到的标记数量；<br><br>- 单位：弧度；<br><br>- 描述标记相对于相机的朝向 |
| `tvecs`        | np.float32 (n,1,3)     | 每个标记的平移向量：<br><br>- 单位：米；<br><br>- 描述标记在相机坐标系中的位置（tvec 的 Z 值≈相机到标记的直线距离）              |

```python
img = aruco.drawAxis(
    image,
    cameraMatrix,
    distCoeffs,
    rvec,
    tvec,
    length,
    thickness=1
)
```
绘制坐标轴


```python
import cv2 as cv
import numpy as np

K = np.float32([[911.29871254, 0, 639.91831779],
                     [0, 911.73955616, 338.96825192],
                     [0, 0, 1]])
D = np.float32([-2.61477667e-01, 1.42009256e+00, 2.54998258e-03,
                     -6.17125588e-03, -2.62607092e+00])


# 生成aruco字典，检测参数，创建检测器
aruco_dict = cv.aruco.getPredefinedDictionary(cv.aruco.DICT_4X4_100)
aruco_param = cv.aruco.DetectorParameters()
detector = cv.aruco.ArucoDetector(aruco_dict, aruco_param)

# 从视频中
cap = cv.VideoCapture(0)
if not cap.isOpened():
    exit('无法打卡摄像头')
while cv.waitKey(50) != ord('q'):
    ret, frame = cap.read()
    if not ret:
        continue
    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    frame_c = frame.copy()

    # 检测标记
    corners, ids, _ = detector.detectMarkers(gray)
    if ids is not None:
        # 可视化标记位置
        cv.aruco.drawDetectedMarkers(frame_c, corners, ids, (0, 255, 0))

        # 估计并可视化位姿
        rvecs, tvecs, _ = cv.aruco.estimatePoseSingleMarkers(corners, 0.1, K, D)
        for rvec, tvec in zip(rvecs, tvecs):
            cv.drawFrameAxes(frame_c, K, D, rvec, tvec, 0.2, 2)

    cv.imshow('video', frame_c)

cap.release()
cv.destroyAllWindows()
```